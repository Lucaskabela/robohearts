{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZLX5bfvuvJBg",
    "outputId": "7d71368e-d091-484f-b07e-1bf29b0ccf22"
   },
   "outputs": [],
   "source": [
    "# !unzip robohearts.zip\n",
    "#%cd robohearts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQFmIAWEj30z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import multiprocessing\n",
    "from gymhearts.Hearts import *\n",
    "from gymhearts.Agent.human import Human\n",
    "from gymhearts.Agent.random_agent import RandomAgent\n",
    "from gymhearts.Agent.monte_carlo_nn import MonteCarloNN\n",
    "from gymhearts.Agent.hand_approx import save_model\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "xsPqUNz_j307",
    "outputId": "c1591ddd-a48d-4aed-bc1b-64c392711308"
   },
   "outputs": [],
   "source": [
    "NUM_TESTS = 10\n",
    "NUM_EPISODES = 1000\n",
    "TRAINING_ITERS = 10000\n",
    "MAX_SCORE = 100\n",
    "\n",
    "run_train = True\n",
    "\n",
    "playersNameList = ['MonteCarlo', 'Rando', 'Randy', 'Randall']\n",
    "agent_list = [0, 0, 0, 0]\n",
    "\n",
    "# Human vs Random\n",
    "# in_hand, in_play, played_cards, won_cards, scores\n",
    "mc_config = {\n",
    "    'print_info' : False,\n",
    "    'epsilon' : .05,\n",
    "    'gamma' : .9,\n",
    "    'alpha': 1e-4,\n",
    "    'feature_list' : ['in_hand', 'in_play']\n",
    "}\n",
    "agent_list[0] = MonteCarloNN(playersNameList[0], mc_config)\n",
    "agent_list[1] = RandomAgent(playersNameList[1], {'print_info' : False})\n",
    "agent_list[2] = RandomAgent(playersNameList[2], {'print_info' : False})\n",
    "agent_list[3] = RandomAgent(playersNameList[3], {'print_info' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt8mEPEQj31D"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021f4ffdbec9447a834c3d1e1bb40935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "array() missing required argument 'object' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-03c636b41bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayName\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDo_Action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/robohearts/gymhearts/Agent/monte_carlo_nn.py\u001b[0m in \u001b[0;36mDo_Action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mchoose_card\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2c'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_greedy_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mchoose_card\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_valid_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/robohearts/gymhearts/Agent/monte_carlo_nn.py\u001b[0m in \u001b[0;36mepsilon_greedy_selection\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_valid_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Return the value of a hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/robohearts/gymhearts/Agent/monte_carlo_nn.py\u001b[0m in \u001b[0;36mgreedy_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mobs_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hand'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msucc_hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mobs_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'currentTrick'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'playerName'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'card'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcard\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0msucc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msucc_val\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_succ_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mbest_move\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_succ_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/robohearts/gymhearts/Agent/monte_carlo_nn.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         ft = get_features(observation, feature_list=self.FT_LIST, \n\u001b[0;32m--> 129\u001b[0;31m             played_cards=self.played_cards, won_cards=self.won_cards, scores=self.scores)\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/robohearts/gymhearts/Agent/hand_approx.py\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(observation, feature_list, played_cards, won_cards, scores)\u001b[0m\n\u001b[1;32m    104\u001b[0m '''\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'in_hand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayed_cards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwon_cards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'in_hand'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: array() missing required argument 'object' (pos 1)"
     ]
    }
   ],
   "source": [
    "# TRAIN THE MONTE CARLO AGENT\n",
    "env = gym.make('Hearts_Card_Game-v0')\n",
    "env.__init__(playersNameList, MAX_SCORE)\n",
    "if run_train:\n",
    "    for trn_episode in tqdm_notebook(range(TRAINING_ITERS)):\n",
    "        observation = env.reset()\n",
    "        history = []\n",
    "        while True:\n",
    "            #env.render()\n",
    "\n",
    "            now_event = observation['event_name']\n",
    "            IsBroadcast = observation['broadcast']\n",
    "            # update my agent before clearing state\n",
    "            if now_event == 'RoundEnd':\n",
    "                agent_list[0].update_weights(history, -reward['MonteCarlo'])\n",
    "                history = []\n",
    "            if now_event == 'GameOver':\n",
    "                  break\n",
    "            if observation['event_name']=='PlayTrick' and observation['data']['playerName'] == 'MonteCarlo':\n",
    "                # don't add score, they don't change till at end of round\n",
    "                history.append((observation, agent_list[0].played_cards, agent_list[0].won_cards))\n",
    "\n",
    "            action = None\n",
    "            if IsBroadcast == True:\n",
    "                for agent in agent_list:\n",
    "                    agent.Do_Action(observation)\n",
    "            else:\n",
    "                playName = observation['data']['playerName']\n",
    "                for agent in agent_list:\n",
    "                    if agent.name == playName:\n",
    "                        action = agent.Do_Action(observation)\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    save_model(agent_list[0].nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZX6qQi9blVFk"
   },
   "outputs": [],
   "source": [
    "def run_test(num_won):\n",
    "    # Weird hack to make progress bars render properly\n",
    "    print(' ', end='', flush=True)\n",
    "    for i_ep in tqdm_notebook(range(NUM_EPISODES)):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            now_event = observation['event_name']\n",
    "            IsBroadcast = observation['broadcast']\n",
    "            action = None\n",
    "            if IsBroadcast == True:\n",
    "                for agent in agent_list:\n",
    "                    agent.Do_Action(observation)\n",
    "            else:\n",
    "                playName = observation['data']['playerName']\n",
    "                for agent in agent_list:\n",
    "                    if agent.name == playName:\n",
    "                        action = agent.Do_Action(observation)\n",
    "            if now_event == 'GameOver':\n",
    "                num_won += int(observation['data']['Winner'] == 'MonteCarlo')\n",
    "                break\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    return num_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Qn-MK5D5pBUw",
    "outputId": "24766394-6d68-4b8b-b7f9-8d058443c268"
   },
   "outputs": [],
   "source": [
    "# Uncomment this line for pretrained weights\n",
    "# weights = [-0.46031637, -1.02296217, -1.64597146, 0.50871499, 0.05907032, -0.04527117,\n",
    "# -2.67590788, -1.78400492, -0.08667306, 0.48108891, 0.66066313, -1.57675411,\n",
    "# -0.56494518, -0.07736412, -0.3257198, -0.65003209, -0.63740714, 0.44494984,\n",
    "# -0.1545964, 0.67457139, 2.31472314, 0.8694452, -2.29173301, 0.52783125,\n",
    "# -0.86950875, -1.77655688, -3.29970913, -0.242993, -1.57548922, -1.34238258,\n",
    "# 0.36816378, -3.23065985, -0.07919411, -2.1089143, -3.12815169, -0.74580836,\n",
    "# 0.98398675, -0.75271283, -0.81051661, -0.60567687, -3.42010519, -0.63186969,\n",
    "# -2.02352157, -0.27534069, -0.28736574, -1.15836776, -3.28679005, -0.33767846,\n",
    "# -0.41568405, 0.2782292, -1.23761129, -1.80559854]\n",
    "\n",
    "# EVALUATE THE MONTE CARLO AGENT\n",
    "    \n",
    "env = gym.make('Hearts_Card_Game-v0')\n",
    "env.__init__(playersNameList, MAX_SCORE)\n",
    "agent_list[0] = MonteCarloNN(playersNameList[0], params={'nn_path': 'mlp', 'in_hand': True, 'in_play': True,})\n",
    "mc_wins = [0] * NUM_TESTS        \n",
    "pool = multiprocessing.Pool(processes=NUM_TESTS)\n",
    "mc_wins = pool.map(run_test, mc_wins)\n",
    "print(mc_wins)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7y1WTLbOEYw-"
   },
   "outputs": [],
   "source": [
    "def run_test_2(num_won):\n",
    "    # Weird hack to make progress bars render properly\n",
    "    print(' ', end='', flush=True)\n",
    "    for i_ep in tqdm_notebook(range(NUM_EPISODES)):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            now_event = observation['event_name']\n",
    "            IsBroadcast = observation['broadcast']\n",
    "            action = None\n",
    "            if IsBroadcast == True:\n",
    "                for agent in agent_list:\n",
    "                    agent.Do_Action(observation)\n",
    "            else:\n",
    "                playName = observation['data']['playerName']\n",
    "                for agent in agent_list:\n",
    "                    if agent.name == playName:\n",
    "                        action = agent.Do_Action(observation)\n",
    "            if now_event == 'GameOver':\n",
    "                num_won += int(observation['data']['Winner'] == 'Randman')\n",
    "                break\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    return num_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-b3qOJsbh9pf"
   },
   "outputs": [],
   "source": [
    "# EVALUATE THE RANDOM AGENT\n",
    "env = gym.make('Hearts_Card_Game-v0')\n",
    "env.__init__(playersNameList, MAX_SCORE)\n",
    "playersNameList[0] = 'Randman'\n",
    "agent_list[0] = RandomAgent(playersNameList[0])\n",
    "rand_wins = [0] * NUM_TESTS\n",
    "        \n",
    "pool = multiprocessing.Pool(processes=NUM_TESTS)\n",
    "rand_wins = pool.map(run_test_2, rand_wins)\n",
    "print(rand_wins)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kXj1tPLnR7F"
   },
   "outputs": [],
   "source": [
    "print(f\"Monte Carlo won {sum(mc_wins)/len(mc_wins)} times on average :: {str(mc_wins)}\")\n",
    "print(f\"Random won {sum(rand_wins)/len(rand_wins)} times on average :: {str(rand_wins)}\")\n",
    "# print(f\"\\n\\nThe Monte Carlo weights are: {str(weights)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mc_nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
