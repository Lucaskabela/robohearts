{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setting\n",
    "\n",
    "- Recent developments in reinforcement learning  fueled by deep networks with raw state representations passed as input (see AlphaGo, Atari DQN)\n",
    "- Growing problems settings and architectures lead to explosion of parameters\n",
    "- Reasearchers do not want to handcraft features, or waste compute time on large models that are ineffective.  \n",
    "\n",
    "# Problem Statement\n",
    "\"Can a reduced set of raw state features serve as a proxy, or even improve the performance of reinforcement learning models compatable with neural networks\"\n",
    "\n",
    "- Domain is Hearts, as it decomposes into small number of raw features\n",
    "- 3 experiments:  \n",
    "    - First, investigate setting and function approximation of raw features\n",
    "    - Second, investiage performance of approximation with reduced combinations of raw state features \n",
    "    - Lastly, attempt to improve on upperbound in Hearts with RL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Framework\n",
    "## Game of Hearts\n",
    "Game is trick winning, where highest card of the leading suit wins a trick\n",
    "\n",
    "*Goal*: win as few points as possible (Hearts are 1 point, Qs is 13)\n",
    "\n",
    "Winner of the last suit leads, cannot lead with hearts unless they have been broken.  Must play the card of the leading suit (if you have it)\n",
    "\n",
    "## Formulation as an MDP:\n",
    " - States:  All combinations of hands, cards played, and scores.  Decomposes into:\n",
    "    1.  cards in hand (in-hand)\n",
    "    2.  card in play during current trick (in-play)\n",
    "    3.  cards which have been played in current round (played-cards)\n",
    "    4.  cards which have been won by each player (cards-won)\n",
    "    5.  scores of each player (scores)\n",
    "\n",
    " - Action: Choose a card to play from your hand (must follow the rules!?!), passing cards X\n",
    " - Reward: Negative of points won (agent wants less points)\n",
    " - Time, space both discrete (trick is time step, action space discrete)\n",
    " - POMDP, Multiagent (relax these assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmH_1vbdo9II"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import multiprocessing\n",
    "from gymhearts.Hearts import *\n",
    "from gymhearts.Agent.agent_random import RandomAgent\n",
    "from gymhearts.Agent.agent_mc_simple import MonteCarlo\n",
    "from gymhearts.Agent.agent_mc_nn import MonteCarloNN\n",
    "from gymhearts.Agent.agent_reinforce import REINFORCE_Agent\n",
    "from gymhearts.Agent.utils_env import *\n",
    "from gymhearts.Agent.utils_nn import *\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D53gd2nUo-nk"
   },
   "outputs": [],
   "source": [
    "# ---------- EVALUATE MC SIMPLE AGENT --------------\n",
    "\n",
    "# Number of episodes to run during model evaluation\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "# Number of model evaluations to average together\n",
    "NUM_TESTS = 1\n",
    "\n",
    "# Max score for players to win the game\n",
    "MAX_SCORE = 100\n",
    "\n",
    "# Run testing on a random agent for comparison\n",
    "run_random = False\n",
    "\n",
    "# Name of the file that is saved :: {model_name}.th\n",
    "model_name = 'final_mc_simple'\n",
    "\n",
    "# Evaluation parameters for testing\n",
    "mc_simple_config = {\n",
    "    'print_info' : False,\n",
    "    'load_model' : model_name\n",
    "}\n",
    "\n",
    "playersNameList = ['MonteCarlo', 'Rando', 'Randy', 'Randall']\n",
    "agent_list = [0, 0, 0, 0]\n",
    "\n",
    "agent_list[0] = MonteCarlo(playersNameList[0], mc_simple_config)\n",
    "agent_list[1] = RandomAgent(playersNameList[1], {'print_info' : False})\n",
    "agent_list[2] = RandomAgent(playersNameList[2], {'print_info' : False})\n",
    "agent_list[3] = RandomAgent(playersNameList[3], {'print_info' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qn-MK5D5pBUw"
   },
   "outputs": [],
   "source": [
    "# Function to test mc simple model with multiprocessing\n",
    "def run_test(num_won):\n",
    "    # Weird hack to make progress bars render properly\n",
    "    print(' ', end='', flush=True)\n",
    "    for i_ep in tqdm_notebook(range(NUM_EPISODES)):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            now_event = observation['event_name']\n",
    "            IsBroadcast = observation['broadcast']\n",
    "            action = None\n",
    "            if IsBroadcast == True:\n",
    "                for agent in agent_list:\n",
    "                    agent.Do_Action(observation)\n",
    "            else:\n",
    "                playName = observation['data']['playerName']\n",
    "                for agent in agent_list:\n",
    "                    if agent.name == playName:\n",
    "                        action = agent.Do_Action(observation)\n",
    "            if now_event == 'GameOver':\n",
    "                num_won += int(observation['data']['Winner'] == 'MonteCarlo')\n",
    "                break\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    return num_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3kXj1tPLnR7F",
    "outputId": "68fec8f7-1d64-4d15-bcc3-7b979d093f32"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Hearts_Card_Game-v0')\n",
    "env.__init__(playersNameList, MAX_SCORE)\n",
    "\n",
    "mc_wins = [0] * NUM_TESTS\n",
    "   \n",
    "pool = multiprocessing.Pool(processes=NUM_TESTS)\n",
    "mc_wins = pool.map(run_test, mc_wins)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(f\"Monte Carlo Simple won {sum(mc_wins)/len(mc_wins)} times on average :: {str(mc_wins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- EVALUATE MC NN AGENT ---------------\n",
    "\n",
    "# Features to include in model :: [in_hand, in_play, played_cards, won_cards, scores]\n",
    "feature_list = ['in_hand', 'in_play']\n",
    "\n",
    "# Name of the file that is saved :: {model_name}.th\n",
    "model_name = 'final_mc_nn'\n",
    "\n",
    "# Evaluation parameters for testing\n",
    "mc_nn_config = {\n",
    "    'print_info' : False,\n",
    "    'load_model' : model_name,\n",
    "    'feature_list' : feature_list\n",
    "}\n",
    "\n",
    "agent_list[0] = MonteCarloNN(playersNameList[0], mc_nn_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test mc nn model with multiprocessing\n",
    "def run_test(num_won):\n",
    "    # Weird hack to make progress bars render properly\n",
    "    print(' ', end='', flush=True)\n",
    "    for i_ep in tqdm_notebook(range(NUM_EPISODES)):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            now_event = observation['event_name']\n",
    "            IsBroadcast = observation['broadcast']\n",
    "            action = None\n",
    "            if IsBroadcast == True:\n",
    "                for agent in agent_list:\n",
    "                    agent.Do_Action(observation)\n",
    "            else:\n",
    "                playName = observation['data']['playerName']\n",
    "                for agent in agent_list:\n",
    "                    if agent.name == playName:\n",
    "                        action = agent.Do_Action(observation)\n",
    "            if now_event == 'GameOver':\n",
    "                num_won += int(observation['data']['Winner'] == 'MonteCarlo')\n",
    "                break\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    return num_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Hearts_Card_Game-v0')\n",
    "env.__init__(playersNameList, MAX_SCORE)\n",
    "\n",
    "mc_wins = [0] * NUM_TESTS \n",
    "\n",
    "pool = multiprocessing.Pool(processes=NUM_TESTS)\n",
    "mc_wins = pool.map(run_test, mc_wins)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(f\"Monte Carlo NN won {sum(mc_wins)/len(mc_wins)} times on average :: {str(mc_wins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- EVALUATE REINFORCE AGENT ---------------\n",
    "\n",
    "# Features to include in model :: [in_hand, in_play, played_cards, won_cards, scores]\n",
    "feature_list = ['in_hand', 'in_play']\n",
    "\n",
    "# Name of the file that is saved :: {model_name}.th\n",
    "model_name = 'final_reinforce'\n",
    "\n",
    "# Evaluation parameters for testing\n",
    "reinforce_config = {\n",
    "    'print_info' : False,\n",
    "    'load_model' : model_name,\n",
    "    'feature_list' : feature_list\n",
    "}\n",
    "\n",
    "playersNameList = ['REINFORCE', 'Rando', 'Randy', 'Randall']\n",
    "agent_list[0] = REINFORCE_Agent(playersNameList[0], reinforce_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test reinforce model with multiprocessing\n",
    "def run_test(num_won):\n",
    "    # Weird hack to make progress bars render properly\n",
    "    print(' ', end='', flush=True)\n",
    "    for i_ep in tqdm_notebook(range(NUM_EPISODES)):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            now_event = observation['event_name']\n",
    "            IsBroadcast = observation['broadcast']\n",
    "            action = None\n",
    "            if IsBroadcast == True:\n",
    "                for agent in agent_list:\n",
    "                    agent.Do_Action(observation)\n",
    "            else:\n",
    "                playName = observation['data']['playerName']\n",
    "                for agent in agent_list:\n",
    "                    if agent.name == playName:\n",
    "                        action = agent.Do_Action(observation)\n",
    "            if now_event == 'GameOver':\n",
    "                num_won += int(observation['data']['Winner'] == 'REINFORCE')\n",
    "                break\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    return num_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Hearts_Card_Game-v0')\n",
    "env.__init__(playersNameList, MAX_SCORE)\n",
    "\n",
    "reinforce_wins = [0] * NUM_TESTS\n",
    "\n",
    "pool = multiprocessing.Pool(processes=NUM_TESTS)\n",
    "reinforce_wins = pool.map(run_test, reinforce_wins)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(f\"REINFORCE won {sum(reinforce_wins)/len(reinforce_wins)} times on average :: {str(reinforce_wins)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "## Linear v NonLinear\n",
    " - NonLinear performs 5.12% difference better, is statistically significant\n",
    " - Both had hyperparameters studied, nonlinear took 10 times as long\n",
    "\n",
    "## Feature Combinations\n",
    " - 1, 2 is the best pair (but not significant to all features) at 35% improvement\n",
    " - 1, 2, 5 is the best triplet, significant\n",
    " - 1, 2 is nearly half the size of full set, trains significantly faster less space\n",
    "\n",
    "## REINFORCE\n",
    " - Lastly, thought the domain was more inclined to policy gradient\n",
    " - Used reinforce, 1, 2 reached ~50% win rate\n",
    "\n",
    "# Conclusions:\n",
    " - Reduced raw feature sets can serve as proxy, and even perform better than the full state \n",
    " - Researchers can use smaller initial models, explore raw state space without time consuming feature engineering\n",
    " - Room for future research:  Lots of assumptions are very strict, trained vs random, what is the best optimal can do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test_hearts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
